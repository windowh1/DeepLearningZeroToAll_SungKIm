---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.4
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Lab 11-2

```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np
import matplotlib.pyplot as plt

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

nb_classes = 10

y_train_new = np.zeros((len(y_train), nb_classes))       #60000 * 10 배열 생성
for i in range(len(y_train_new)):                        
  y_train_new[i, y_train[i]] = 1                          #one-hot encoding 

y_test_new = np.zeros((len(y_test), nb_classes))       #60000 * 10 배열 생성
for i in range(len(y_test_new)):                        
  y_test_new[i, y_test[i]] = 1  

```

## Conv layer 1

```python
X = tf.placeholder(tf.float32, [None, 784])
X_img = tf.reshape(X, [-1, 28, 28, 1])
Y = tf.placeholder(tf.float32, [None, nb_classes])

W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev = 0.01))
L1 = tf.nn.conv2d(X_img, W1, strides = [1, 1, 1, 1], padding = 'SAME')
L1 = tf.nn.relu(L1)
L1 = tf.nn.max_pool(L1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')
```

## Conv layer 2

```python
W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev = 0.01))
L2 = tf.nn.conv2d(L1, W2, strides = [1, 1, 1, 1], padding = 'SAME')
L2 = tf.nn.relu(L2)
L2 = tf.nn.max_pool(L2, ksize = [1, 2, 2, 1], strides = [1, 2, 2 ,1], padding = 'SAME')
L2 = tf.reshape(L2, [-1, 7 * 7 * 64])
```

## Fully Connected (Dense) layer

```python
lr = 0.001

with tf.variable_scope("d") as scope:
    W3 = tf.get_variable("W3", shape = [7 * 7 * 64, nb_classes], initializer = tf.truncated_normal_initializer(stddev = 0.1))
    b = tf.Variable(tf.random_normal([10]))
    hypothesis = tf.matmul(L2, W3) + b

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))
optimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(cost)
```

```python
x_train = x_train.reshape([-1, 784])
x_test = x_test.reshape([-1, 784])

sess = tf.Session()
sess.run(tf.global_variables_initializer())

training_epochs = 15
batch_size = 100
total_batch = int(len(x_train) / batch_size)

for epoch in range(training_epochs):
    avg_cost = 0
    for i in range(total_batch):
        batch_xs = x_train[i * batch_size : (i+1) * batch_size]
        batch_ys = y_train_new[i * batch_size : (i+1) * batch_size]
        _, c = sess.run([optimizer, cost], feed_dict = {X : batch_xs, Y : batch_ys})
        avg_cost = avg_cost + c / total_batch
    print("Epoch:", epoch, "Cost:", avg_cost)

print("Learning Finished")

correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print("Accuracy:", sess.run(accuracy, feed_dict = {X : x_test, Y: y_test_new}))
```

## Deep CNN

```python
rt = tf.placeholder(tf.float32)

W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev = 0.01))
L1 = tf.nn.conv2d(X_img, W1, strides = [1, 1, 1, 1], padding = 'SAME')
L1 = tf.nn.relu(L1)
L1 = tf.nn.max_pool(L1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')
L1 = tf.nn.dropout(L1, rate = rt)
# Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.

W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev = 0.01))
L2 = tf.nn.conv2d(L1, W2, strides = [1, 1, 1, 1], padding = 'SAME')
L2 = tf.nn.relu(L2)
L2 = tf.nn.max_pool(L2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')
L2 = tf.nn.dropout(L2, rate = rt)

W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev = 0.01))
L3 = tf.nn.conv2d(L2, W3, strides = [1, 1, 1, 1], padding = 'SAME')
L3 = tf.nn.relu(L3)
L3 = tf.nn.max_pool(L3, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')
L3 = tf.nn.dropout(L3, rate = rt)
L3 = tf.reshape(L3, [-1, 4 * 4 * 128])

with tf.variable_scope('ad') as scope:
    W4 = tf.get_variable("W4", shape = [4 * 4* 128, 625], initializer = tf.truncated_normal_initializer(stddev = 0.1))
    b4 = tf.Variable(tf.random_normal([625]))
    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)
    L4 = tf.nn.dropout(L4, rate = rt)

with tf.variable_scope('aad') as scope:
    W5 = tf.get_variable("W5", shape = [625, nb_classes], initializer = tf.truncated_normal_initializer(stddev = 0.1))
    b5 = tf.Variable(tf.random_normal([nb_classes]))
    hypothesis = tf.matmul(L4, W5) + b5

lr = 0.0005

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))
optimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(cost)

sess.run(tf.global_variables_initializer())

for epoch in range(training_epochs):
    avg_cost = 0
    for i in range(total_batch) :
        batch_xs = x_train[i * batch_size : (i+1) * batch_size]
        batch_ys = y_train_new[i * batch_size : (i+1) * batch_size]
        _, c = sess.run([optimizer, cost], feed_dict = {X : batch_xs, Y : batch_ys, rt : 0.2})
        avg_cost = avg_cost + c / total_batch
    print("Epoch:", epoch, "Cost:", avg_cost)

correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print("Accuracy:", sess.run(accuracy, feed_dict = {X : x_test, Y : y_test_new, rt : 0.0}))
```
